{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcf0fca",
   "metadata": {},
   "source": [
    "# LTR Test Task\n",
    "## Jeff Wagg, March 2023\n",
    "\n",
    "Classical search algorithms generate candidate matches from an input query in order to generate 'relevant' results. Here, we are given a large training dataset of queries and resulting links to images and metadata such as text and title, some of which are actually relevant to the original search. We want to develop a model which will allow us to make predictions for whether an image is relevant, or not based on a set of associated features (keywords, image characteristics, etc.). To achieve this we will attempt to use a combination of image recognition for classifying the images returned from the query, keyword search, and Natural Language Processing (NLP) to refine and interpret the text query. \n",
    "\n",
    "Once a model has been developed, this will be applied to a smaller test dataset in order to classify images as relevant, or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcaa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by importing some useful packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import requests, os, json, lxml\n",
    "from PIL import Image\n",
    "from pytesseract import pytesseract\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import classification_report\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use tesseract to extract words from images \n",
    "path_to_tesseract = '/usr/local/bin/tesseract'\n",
    "pytesseract.tesseract_cmd = path_to_tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fdd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to set the header for url searches so that our queries are not blocked by the sites as bots\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "    \"Accept\":\"text/html,application/xhtml+xml,application/xml; q=0.9,image/webp,*/*;q=0.8\",'Accept-Language': 'en-US,en;q=0.8'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357dd6d8",
   "metadata": {},
   "source": [
    "## Load the Data and Perform some Exploratory Data Analysis and Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "file_train = 'train.feather'\n",
    "file_test = 'test.feather'\n",
    "\n",
    "df_train = pd.read_feather(file_train)\n",
    "df_train = df_train.fillna(0) # fill na entries with '0'\n",
    "print(\"Training set loaded. The number of samples is: \",len(df_train.index))\n",
    "\n",
    "df_test = pd.read_feather(file_test)\n",
    "df_test = df_test.fillna(0) # fill na entries with '0'\n",
    "print(\"Testing set loaded. The number of samples is: \",len(df_test.index))\n",
    "\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63aea25",
   "metadata": {},
   "source": [
    "We see that there are 23 columns in the training dataset. We want to build a model that can be used to assess which of the features is most relevant for determining whether a particular search outcome is relevant, or not. I initially hypothesize that some combination of 'title', 'src' (url of image), 'text_tag', and 'text' will be the most relevant for predicting the outcome, or target variable: 'is_relevant'. \n",
    "\n",
    "Now, let's find the total number of unique searches in the training set and the average number, k, of candidates generated for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c635428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of searches is: \",df_train['query'].nunique())\n",
    "\n",
    "queries = df_train['query'].unique()\n",
    "kavg = 0.\n",
    "for q in queries:\n",
    "    df_tmp = df_train.loc[df_train['query'] == q]\n",
    "    k = len(df_tmp.index)\n",
    "    #print(q,\" number of k:\",k)\n",
    "    kavg += k\n",
    "kavg = kavg / len(queries)\n",
    "print(\"Each search returns an average of \",kavg,\" results\")\n",
    "\n",
    "query_uniq = df_train['query'].unique()\n",
    "print(query_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522468e",
   "metadata": {},
   "source": [
    "We are given a list of features contained in the dataframe that may be useful for this analysis. These include:\n",
    "\n",
    "“id” : unique identifier for an image. We do not expect this to be useful for the predictions. \\\n",
    "“query” : text query, which is used to determine the relevance of an image. Interesting to note that all of the queries are single element. \\\n",
    "“url_page”: webpage where the image is found. The keywords derived from this feature may be used in the model. \\\n",
    "“src” : the source image url of the image, this is the url for the image itself. The keywords extracted from this link are expected to be important for the modelling. We may also be able to use the image itself for predictions.  \\\n",
    "“title”: title of the “url_page”. There may be words in the title which prove useful for the model. \\\n",
    "“alt”: alternate text for the image. Words extracted from here are likely to be useful for making predictions. \\\n",
    "“is_relevant”: 1 if image is relevant to the query, 0 otherwise. This is the target for the analysis. \n",
    "\n",
    "In examining the list of queries, I find that some of these are misspelled. For example, there is a query for 'eldery' which should be written as 'elderly', or a query for 'alltvshows' which should be 'all tv shows'. This will need to be adressed or corrected for the modelling. Some of the queries are in Spanish, so these may also require a translation and accents removed. A quick search of the list of queries reveals a list of more than 100 misspellings for which we propose changes. In order to find the most likely replacement, I run a Bing search with the original query and extract the recommended alternative text. I also tried Google but encountered a security block after too many webscraping calls.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5596a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for q in query_uniq: \n",
    "    # client param could be replaced with firefox or other browser\n",
    "    params = {\n",
    "      'q': q,\n",
    "      'hl': 'en',\n",
    "      'gl': 'us',\n",
    "    }\n",
    "    \n",
    "    #link = 'https://www.google.com/search?q='\n",
    "    link = 'https://www.bing.com/search?q='\n",
    "    html = requests.get(link, headers=headers, params=params).text\n",
    "    newq = q\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        linetags = str(soup.find(\"li\",{\"class\":\"b_algo\"}))\n",
    "        search_word = linetags.split(\"<strong>\")[1].split()[0]\n",
    "        newq = search_word.split(\"</strong>\")[0].split()[0].lower()\n",
    "    except:\n",
    "        print(\"Keeping original query\")\n",
    "        \n",
    "    print(q,\" suggested: \",newq)\n",
    "    df_train['query'] = df_train['query'].replace([q], newq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e219ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some words do not have replacements, or the Bing suggestions do not make sense. I edit these myself\n",
    "df_train['query'] = df_train['query'].replace(['home'], 'homedepot')\n",
    "df_train['query'] = df_train['query'].replace(['jorgenssns'], 'jorgensen')\n",
    "df_train['query'] = df_train['query'].replace(['bennyes'], 'bennys')  \n",
    "df_train['query'] = df_train['query'].replace(['about'], 'ajw')  \n",
    "df_train['query'] = df_train['query'].replace(['tin'], 'thethao24h')  \n",
    "df_train['query'] = df_train['query'].replace(['a'], 'aeiou')  \n",
    "df_train['query'] = df_train['query'].replace(['cryptocurrenct'], 'cryptocurrency')  \n",
    "# technology -> mstm\n",
    "df_train['query'] = df_train['query'].replace(['bloggle'], 'buc')  \n",
    "df_train['query'] = df_train['query'].replace(['leukorrhea'], 'leucorrhea') \n",
    "df_train['query'] = df_train['query'].replace(['sheikh'], 'sheik')\n",
    "df_train['query'] = df_train['query'].replace(['aliminum'], 'aluminum')\n",
    "df_train['query'] = df_train['query'].replace(['arcangels'], 'archangel')\n",
    "df_train['query'] = df_train['query'].replace(['basball'], 'baseball') \n",
    "df_train['query'] = df_train['query'].replace(['distillium'], 'distylium')\n",
    "df_train['query'] = df_train['query'].replace(['ktire'], 'kture')  \n",
    "df_train['query'] = df_train['query'].replace(['neglegence'], 'negligence')  \n",
    "df_train['query'] = df_train['query'].replace(['unfazed'], 'unphased') \n",
    "df_train['query'] = df_train['query'].replace(['bonbas'], 'bombas')\n",
    "df_train['query'] = df_train['query'].replace(['lasik'], 'lazik')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a2e7c",
   "metadata": {},
   "source": [
    "I note that some of the words appear in different languages. This is not taken into account for this analysis except to remove any accents. \n",
    "\n",
    "Here, I read in some of the relevant images to determine if their characteristics can be used in the model as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(0,19):\n",
    "    isin = 0\n",
    "    if df_train['is_relevant'][i] == 1:\n",
    "        try:\n",
    "            r = requests.get(df_train['src'][i],headers=headers)\n",
    "            with open('tmpimg', 'wb') as outfile:\n",
    "                outfile.write(r.content)\n",
    "            if 'svg' in df_train['src'][i]:\n",
    "                drawing = svg2rlg(\"tmpimg\")\n",
    "                renderPM.drawToFile(drawing, \"tmpimg\", fmt=\"PNG\")\n",
    "            img =  Image.open('tmpimg')\n",
    "            text = pytesseract.image_to_string(img)\n",
    "            text = text.replace('\\n','')\n",
    "            img_low = str(text).lower()\n",
    "            #img.show()\n",
    "            if (df_train['query'][i] in img_low):\n",
    "                print(i,\"Yes, query is in image.\")\n",
    "            else:\n",
    "                print(i,\"No, query is not in image\")\n",
    "        except:\n",
    "            print(i,\"Sorry, could not read image: \",df_train['src'][i])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea679b",
   "metadata": {},
   "source": [
    "In the previous cell, we tried reading the images (linked through 'src') to see if any of their characteristics can be used as a model feature. After running Pillow to extract and open only the images associated with '1' for 'is_relevant', I found that many of the images could not be read, in some cases showing 'File not found'. The main issue appears to be due to the sites using security to block potential webscrapers. This was fixed by including the headers defined in an earlier cell if the code. Some of the files are in 'svg' format and could not be read with PIL. These need to be read with 'svg2rlg(IMAGE_NAME)' and then converted to PNG format. \n",
    "\n",
    "We then used tesseract to extract any text found in the image and checked whether the 'query' was in these words. We found that the query was infrequently in the image itself. As such, this is unlikely to be a good model feature, and we will exclude it from here on.  \n",
    "\n",
    "In running these tests while printing out some of the other feature values, I did note that the query text often appears in either the 'alt' text or URL of the relevant image ('src'). I also note that the query phrase appears in the title about 70% of the time, whether the query is relevant, or not. In the next section, I check to see if these matches are a reliable indicator of whether the results are relevant, or not.  \n",
    "\n",
    "Finally, I calculate the fraction of images which are deemed to be relevant. You can see that these make up only ~3.6% of the training sample, meaning that we have unbalanced data which should be factored into the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de756176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Only %\",100.*df_train['is_relevant'].sum()/len(df_train.index),\" of the images are relevant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0585975",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "\n",
    "Given that we have seen that the presence of the search phrase in at least two of the features can give us some insight into whether the resulting image is relevant, or not, we create new features which can be used as input into a machine learning model such as logistic regression. We will create 'isintxt_XXX' columns in the training dataframe, where 'XXX' is the name of the original column, and the values will be '0'- text not present, '>=1' - the number of times the query appears in the text. \n",
    "\n",
    "Note - after running a few tests, we found that running tesseract on the images using a CPU is too slow to search through all of the training images in a reasonable amount of time (~110 hours for all 600k images). Given our previous finding that the query is rarely found in the image when the query is relevant, I decide not to use this as a feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01013183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_train['isintxt_src'] = 0\n",
    "df_train['isintxt_alt'] = 0\n",
    "df_train['isintxt_url'] = 0\n",
    "df_train['isintxt_title'] = 0\n",
    "df_train['isintxt_text'] = 0\n",
    "\n",
    "for i in range(0,len(df_train.index) - 1):  \n",
    "    query_low = str(df_train['query'][i]).lower()\n",
    "    \n",
    "    url_low = str(df_train['url_page'][i]).lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "    if (query_low in url_low):\n",
    "        df_train['isintxt_url'][i] = url_low.count(query_low)\n",
    "    \n",
    "    src_low = str(df_train['src'][i]).lower().replace(\"_\", \"\").replace(\"http://\",\"\").replace(\"https://\",\"\").replace(\"/\",\"\")\n",
    "    if (query_low in src_low):\n",
    "        df_train['isintxt_src'][i] = src_low.count(query_low)\n",
    "    \n",
    "    alt_low = str(df_train['alt'][i]).lower().replace(\" \", \"\").replace(\"'\",'').replace(\"-\",'')\n",
    "    alt_low = unidecode(alt_low)\n",
    "    if (query_low in alt_low):\n",
    "        df_train['isintxt_alt'][i] = alt_low.count(query_low)\n",
    "    \n",
    "    title_low = str(df_train['title'][i]).lower().replace(\" \", \"\").replace(\"'\",'').replace(\"-\",'')\n",
    "    title_low = unidecode(title_low)\n",
    "    if (query_low in title_low):\n",
    "        df_train['isintxt_title'][i] = title_low.count(query_low)\n",
    "        \n",
    "    text_low = str(df_train['text'][i]).lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "    text_low = unidecode(text_low)\n",
    "    if (query_low in text_low):\n",
    "        df_train['isintxt_text'][i] = text_low.count(query_low)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a short check to see if there are any obvious patterns in the occurence of the query text \n",
    "print(\"Query Relevant?   # occurences in:  title Alt SRC Text\")\n",
    "\n",
    "for i in range(0,len(df_train.index) - 1):  \n",
    "    if df_train['is_relevant'][i] == 1:\n",
    "        print(df_train['query'][i],df_train['is_relevant'][i],\"                          \",\n",
    "              df_train['isintxt_title'][i],df_train['isintxt_alt'][i],df_train['isintxt_src'][i],\n",
    "                df_train['isintxt_text'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde09ce",
   "metadata": {},
   "source": [
    "Before doing creating any machine learning models, I want to run a quick check to see whether there are any obvious combinations of features that indicate whether an image is relevant to a search query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144be0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_src_text_rel = 0\n",
    "alt_src_rel = 0\n",
    "alt_text_rel = 0\n",
    "src_text_rel = 0\n",
    "allzeros_rel = 0\n",
    "alt_src_text_irrel = 0\n",
    "alt_src_irrel = 0\n",
    "alt_text_irrel = 0\n",
    "src_text_irrel = 0\n",
    "allzeros_irrel = 0\n",
    "alt_rel = 0\n",
    "src_rel = 0\n",
    "text_rel = 0 \n",
    "alt_irrel = 0\n",
    "src_irrel = 0\n",
    "text_irrel = 0 \n",
    "\n",
    "\n",
    "for i in range(0,len(df_train.index) - 1):  \n",
    "    if ((df_train['isintxt_alt'][i] >=1 ) and (df_train['isintxt_src'][i] >= 1) and (df_train['isintxt_text'][i] >= 1)):\n",
    "        if (df_train['is_relevant'][i] == 1):\n",
    "            alt_src_text_rel += 1\n",
    "        else:\n",
    "            alt_src_text_irrel += 1\n",
    "        \n",
    "    if ((df_train['isintxt_alt'][i] >=1) and (df_train['isintxt_src'][i] >= 1) and df_train['isintxt_text'][i] == 0):\n",
    "        if (df_train['is_relevant'][i] == 1):\n",
    "            alt_src_rel += 1\n",
    "        else:\n",
    "            alt_src_irrel += 1\n",
    "    \n",
    "    if ((df_train['isintxt_alt'][i] >= 1) and (df_train['isintxt_text'][i] >= 1) and df_train['isintxt_src'][i] == 0):\n",
    "        if (df_train['is_relevant'][i] == 1):\n",
    "            alt_text_rel += 1\n",
    "        else:\n",
    "            alt_text_irrel += 1\n",
    "    \n",
    "    if ((df_train['isintxt_text'][i] >= 1) and (df_train['isintxt_src'][i] >= 1) and df_train['isintxt_alt'][i] == 0):\n",
    "        if (df_train['is_relevant'][i] == 1):\n",
    "            src_text_rel += 1\n",
    "        else:\n",
    "            src_text_irrel += 1\n",
    "       \n",
    "    if (df_train['isintxt_alt'][i] == df_train['isintxt_src'][i] == 0 and df_train['isintxt_text'][i] >= 1):\n",
    "        if (df_train['is_relevant'][i] == 1):\n",
    "            text_rel += 1\n",
    "        else:\n",
    "            text_irrel += 1\n",
    "       \n",
    "    if (df_train['isintxt_src'][i] == df_train['isintxt_text'][i] == 0 and df_train['isintxt_alt'][i] >= 1):\n",
    "        if (df_train['is_relevant'][i] == 1):\n",
    "            alt_rel += 1\n",
    "        else:\n",
    "            alt_irrel += 1\n",
    "       \n",
    "    if (df_train['isintxt_alt'][i] == df_train['isintxt_text'][i] == 0 and df_train['isintxt_src'][i] >= 1):\n",
    "        if (df_train['is_relevant'][i] == 1):\n",
    "            src_rel += 1\n",
    "        else:\n",
    "            src_irrel += 1\n",
    "       \n",
    "    if (df_train['isintxt_alt'][i] == df_train['isintxt_src'][i] == df_train['isintxt_text'][i] == 0):\n",
    "        if (df_train['is_relevant'][i] == 1):\n",
    "            allzeros_rel += 1\n",
    "        else:\n",
    "            allzeros_irrel += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35db17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_text_rel / (alt_text_rel + alt_text_irrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5314397",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_isrelev = 0\n",
    "num_notrelev = 0\n",
    "num_insrcalt_rel = 0\n",
    "num_insrcalt_notrel = 0\n",
    "\n",
    "for i in range(0, len(df_train.index) - 1):\n",
    "    if df_train['is_relevant'][i] == 1:\n",
    "        num_isrelev += 1\n",
    "        if ((df_train['isintxt_text'][i] >= 1) and (df_train['isintxt_src'][i] >= 1) and (df_train['isintxt_alt'][i] >= 1)):\n",
    "            num_insrcalt_rel += 1\n",
    "    else:\n",
    "        num_notrelev += 1\n",
    "        if ((df_train['isintxt_text'][i] >= 1) and (df_train['isintxt_src'][i] >= 1) and (df_train['isintxt_alt'][i] >= 1)):\n",
    "            num_insrcalt_notrel += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8768899",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fraction of relevant image where query found in alt, src and text:\",num_insrcalt_rel / num_isrelev)\n",
    "print(\"Fraction of irrelevant image where query found in alt, arc and text:\",num_insrcalt_notrel / num_notrelev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec86a3d7",
   "metadata": {},
   "source": [
    "These results are interesting as they suggest that finding the 'query' in some of the other fields can help to distinguish between whether the image is relevant, or not. For example, finding the query in either 'title' or 'url' does not help, as the probability is the same, irrespective of the relevance (appearing in 70% of the titles, and 60% of the URLs). However, in the case of 'alt', 'src' and 'text', the fraction of times that the query appears is about 5.5x higher when the image is relevant. This suggests that these are the most important features for our models. When the query is relevant, it appears in either 'alt' or 'src' or 'text' about 68% of the time, while this is true for only 22% of the cases where the image is irrelevant. Changing the OR condition to an AND, we find that there is only a 2% chance that the image is irrelevant, while this happens in 12% of the relevant images.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522eb3cb",
   "metadata": {},
   "source": [
    "## Machine Learning Model \n",
    "\n",
    "Now that we have defined some plausible model features, we will attempt to develop machine learning models that can use these features to predict the target variable, 'is_relevant'. I attempted to implement three models, 1) Logistic Regression, 2) Poisson Regression, and 3) k-Nearest Neighbour (kNN). \n",
    "\n",
    "I first split the training data into testing (20%) and training (80%) data sets. The models were fit to the training set and then applied to the test data to verify performance. The first two models proved to be unsuccessful, and I was unable to find model parameters that led to any success in predicting when an image would be relevant based on the presence of the query text in any of the features. I have therefore excluded the Logistic Regression and the Poisson Regresson models from the code below. However, the kNN model did show some promise as it is able to associate a probability rather than a binary predictions that the image might be relevant based on the feature variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['isintxt_alt','isintxt_src','isintxt_text','isintxt_title','isintxt_url']\n",
    "X = df_train[feature_cols] # Features\n",
    "y = df_train['is_relevant']\n",
    "\n",
    "# split the training data into test and training sub-samples. Standard 80/20 rule of thumb\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "##### fit the model #####\n",
    "knn_model = KNeighborsRegressor(n_neighbors=44)\n",
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aecb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# try a grid search to find the best value of 'k'- number of neighbours to use for classification\n",
    "parameters = {\"n_neighbors\": range(33, 45)}\n",
    "gridsearch = GridSearchCV(KNeighborsRegressor(), parameters)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "gridsearch.best_params_\n",
    "\n",
    "# We find that 19 neighbours gives the lowest root mean squared error in the grid search when using binary features\n",
    "# when we let the features indicate the number of matches, the number increases to 44 neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d202597",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# make predictions using the training subsample to see how well the model fits \n",
    "#y_pred = logreg.predict(X_test)\n",
    "train_preds = knn_model.predict(X_train)\n",
    "mse = mean_squared_error(y_train, train_preds)\n",
    "rmse = sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56597ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# make predictions using the testing subsample to see how well the model fits \n",
    "test_preds = knn_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, test_preds)\n",
    "rmse = sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7814d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ymean_rel = 0.\n",
    "ymean_irr = 0.\n",
    "\n",
    "for idx, yi in enumerate(y_test):\n",
    "    print(yi,test_preds[idx])\n",
    "    if yi == 1:\n",
    "        ymean_rel += test_preds[idx]\n",
    "    if yi == 0:\n",
    "        \n",
    "        ymean_irr += test_preds[idx]\n",
    "\n",
    "\n",
    "print(\"Avg of relevant: \",ymean_rel/len(y_test[y_test == 1]),\n",
    "      \"Avg of irrelevant:\",ymean_irr/len(y_test[y_test == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96790210",
   "metadata": {},
   "source": [
    "## Apply the Model to the Test Data\n",
    "\n",
    "At this stage we want to apply our kNN machine learning model to the test data set. I first have to preprocess the test data provided, including some feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b868dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of searches in the test data is: \",df_test['query'].nunique())\n",
    "\n",
    "queries_test = df_test['query'].unique()\n",
    "kavg = 0.\n",
    "for q in queries_test:\n",
    "    df_tmp = df_test.loc[df_test['query'] == q]\n",
    "    k = len(df_tmp.index)\n",
    "    kavg += k\n",
    "kavg = kavg / len(queries_test)\n",
    "print(\"Each search returns an average of \",kavg,\" results\")\n",
    "\n",
    "query_uniq_test = df_test['query'].unique()\n",
    "print(query_uniq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59220b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for q in query_uniq_test: \n",
    "    # client param could be replaced with firefox or other browser\n",
    "    params = {\n",
    "      'q': q,\n",
    "      'hl': 'en',\n",
    "      'gl': 'us',\n",
    "    }\n",
    "    \n",
    "    link = 'https://www.bing.com/search?q='\n",
    "    html = requests.get(link, headers=headers, params=params).text\n",
    "    newq = q\n",
    "    try:\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        linetags = str(soup.find(\"li\",{\"class\":\"b_algo\"}))\n",
    "        search_word = linetags.split(\"<strong>\")[1].split()[0]\n",
    "        newq = search_word.split(\"</strong>\")[0].split()[0].lower()\n",
    "    except:\n",
    "        print(\"Keeping original query\")\n",
    "        \n",
    "    print(q,\" suggested: \",newq)\n",
    "    df_test['query'] = df_test['query'].replace([q], newq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_test['isintxt_src'] = 0\n",
    "df_test['isintxt_alt'] = 0\n",
    "df_test['isintxt_url'] = 0\n",
    "df_test['isintxt_title'] = 0\n",
    "df_test['isintxt_text'] = 0\n",
    "\n",
    "for i in range(0,len(df_test.index) - 1):  \n",
    "    query_low = str(df_test['query'][i]).lower()\n",
    "\n",
    "    url_low = str(df_test['url_page'][i]).lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "    if (query_low in url_low):\n",
    "        df_test['isintxt_url'][i] = url_low.count(query_low)\n",
    "    \n",
    "    src_low = str(df_test['src'][i]).lower().replace(\"_\", \"\").replace(\"http://\",\"\").replace(\"https://\",\"\").replace(\"/\",\"\")\n",
    "    if (query_low in src_low):\n",
    "        df_test['isintxt_src'][i] = src_low.count(query_low)\n",
    "    \n",
    "    alt_low = str(df_test['alt'][i]).lower().replace(\" \", \"\").replace(\"'\",'').replace(\"-\",'')\n",
    "    alt_low = unidecode(alt_low)\n",
    "    if (query_low in alt_low):\n",
    "        df_test['isintxt_alt'][i] = alt_low.count(query_low)\n",
    "        \n",
    "    title_low = str(df_test['title'][i]).lower().replace(\" \", \"\").replace(\"'\",'').replace(\"-\",'')\n",
    "    title_low = unidecode(title_low)\n",
    "    if (query_low in title_low):\n",
    "        df_test['isintxt_title'][i] = title_low.count(query_low)\n",
    "        \n",
    "    text_low = str(df_test['text'][i]).lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "    text_low = unidecode(text_low)\n",
    "    if (query_low in text_low):\n",
    "        df_test['isintxt_text'][i] = text_low.count(query_low)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code is for checking the number of times the query appears in some of the most important features\n",
    "print(\"query    is in: title Alt SRC Text    src\")\n",
    "print()\n",
    "\n",
    "numrel = 0\n",
    "for i in range(0,len(df_test.index) - 1):  \n",
    "        print(df_test['query'][i],\"            \",df_test['isintxt_title'][i],df_test['isintxt_alt'][i],df_test['isintxt_src'][i],\n",
    "                df_test['isintxt_text'][i],str(df_test['src'][i]))\n",
    "        numrel = numrel + 1\n",
    "    \n",
    "print(\"Number of relevant images: \",numrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the model to the test data \n",
    "X_test_final = df_test[feature_cols] # Features\n",
    "df_test['is_relevant'] = 0\n",
    "y_test_final = df_test['is_relevant']\n",
    "\n",
    "test_preds_final = knn_model.predict(X_test_final)\n",
    "df_test['is_relevant'] = test_preds_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model output and write to csv file\n",
    "for i in range(0, len(df_test.index)):\n",
    "    print(df_test['id'][i],df_test['is_relevant'][i])\n",
    "    \n",
    "df_test.to_csv('submission.csv', index=False, columns=['id','is_relevant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b838c18",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "After performing some data cleaning and using natural language processing to refine the queries, I have generated new features using the metadata associated with the training dataset. Following this, I attempted to develop three different machine learning models, finding that a simple kNN worked best with 44 neighbours giving the optimal solution. This kNN model provides some insight into whether the images returned by a query are relevant, or not, assigning probabilities based on the number of times the query appears in the various features. \n",
    "\n",
    "With more computing resources, it would be interesting to also try using classification techniques on the images themselves. This is likely to lead to an improvement in the model performance. Once might also consider using word vectors instead of searching for the query words directly in the text features, as this should prove to be computationally faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928919f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
